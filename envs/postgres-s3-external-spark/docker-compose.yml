version: '3'

services:
  airflow:
    image: docker-whirl-airflow:py-${PYTHON_VERSION}-local
    ports:
      - '5000:5000'  # HTTP (Airflow Web UI)
    environment:
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - DEMO_BUCKET
      - AWS_SERVER
      - AWS_PORT
      - PORT_WEB_UI
      - POSTGRES_HOST=postgresdb
      - POSTGRES_PORT
      - POSTGRES_PASSWORD
      - POSTGRES_USER
      - POSTGRES_DB
      - SPARK_HOME=/home/airflow/.local/lib/python3.6/site_packages/pyspark/
    volumes:
      - ${DAG_FOLDER}:/usr/local/airflow/dags/$PROJECTNAME
      - ${ENVIRONMENT_FOLDER}/whirl.setup.d:${WHIRL_SETUP_FOLDER}/env.d/
      - ${DAG_FOLDER}/whirl.setup.d:${WHIRL_SETUP_FOLDER}/dag.d/
      - ${MOCK_DATA_FOLDER}:/mock-data
    depends_on:
      - s3server
      - postgresdb
      - sparkmaster

  s3server:
    build:
      context: ${DOCKER_CONTEXT_FOLDER}/patched-localstack
      dockerfile: Dockerfile
    ports:
      - "4563-4584:4563-4584"
      - "${PORT_WEB_UI-8080}:${PORT_WEB_UI-8080}"
    environment:
      - SERVICES=s3
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - DEMO_BUCKET
      - AWS_SERVER
      - AWS_PORT
      - PORT_WEB_UI

  postgresdb:
    image: postgres:11
    ports:
      - 5432:5432
    environment:
      - POSTGRES_HOST=postgresdb
      - POSTGRES_PORT
      - POSTGRES_PASSWORD
      - POSTGRES_USER
      - POSTGRES_DB

  sparkmaster:
    build:
      context: ${DOCKER_CONTEXT_FOLDER}/aws-spark
      dockerfile: Dockerfile
    environment:
      - SPARK_NO_DAEMONIZE=true
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_SERVER
      - AWS_PORT
    ports:
      - 7077:7077
      - 18080:8080
    entrypoint:
      - /usr/spark/sbin/start-master.sh
#      - --conf
#      - spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
#      - --conf
#      - spark.hadoop.fs.s3a.access.key=${AWS_ACCESS_KEY_ID}
#      - --conf
#      - spark.hadoop.fs.s3a.secret.key=${AWS_SECRET_ACCESS_KEY}
#      - --conf
#      - spark.hadoop.fs.s3a.endpoint=${AWS_SERVER}:${AWS_PORT}
#      - --conf
#      - spark.hadoop.fs.s3a.connection.ssl.enabled=false
#      - --conf
#      - spark.hadoop.fs.s3a.path.style.access=true
#      - --conf
#      - spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    links:
      - s3server:${DEMO_BUCKET}.s3server

  sparkworker:
    build:
      context: ${DOCKER_CONTEXT_FOLDER}/aws-spark
      dockerfile: Dockerfile
    environment:
      - SPARK_NO_DAEMONIZE=true
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_SERVER
      - AWS_PORT
    entrypoint:
      - /usr/spark/sbin/start-slave.sh
      - spark://sparkmaster:7077
#      - --conf
#      - spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
#      - --conf
#      - spark.hadoop.fs.s3a.access.key=${AWS_ACCESS_KEY_ID}
#      - --conf
#      - spark.hadoop.fs.s3a.secret.key=${AWS_SECRET_ACCESS_KEY}
#      - --conf
#      - spark.hadoop.fs.s3a.endpoint=${AWS_SERVER}:${AWS_PORT}
#      - --conf
#      - spark.hadoop.fs.s3a.connection.ssl.enabled=false
#      - --conf
#      - spark.hadoop.fs.s3a.path.style.access=true
#      - --conf
#      - spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    depends_on:
      - sparkmaster
    links:
      - s3server:${DEMO_BUCKET}.s3server

